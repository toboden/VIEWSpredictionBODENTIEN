{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model type\n",
    "estimModel = 'nbinom' #nbinom or hurdle\n",
    "\n",
    "# list of the (prediction) windows\n",
    "max_w = 24\n",
    "window_list = list(range(2, max_w+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import CRPS.CRPS as pscore\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "from scipy.stats import nbinom, poisson\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "## functions for the distribtion models\n",
    "# cdf of the truncated negative binomial distribution\n",
    "def truncNbinomCdf(y, n, p, log=True):\n",
    "\n",
    "    ## error/input handling part one\n",
    "    # n and p have to be greater than zero and p <= 1\n",
    "    if n <= 0 or p <= 0 or p > 1:\n",
    "        if not isinstance(y, (list, np.ndarray)):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.full(len(y), np.nan)\n",
    "\n",
    "    ## calculation\n",
    "    is_scalar = np.isscalar(y)  # check if y is scalar or array\n",
    "\n",
    "    if is_scalar:\n",
    "        y = np.array([y])  # typecast scalar to onedimensional array\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "\n",
    "    f_zero = nbinom.pmf(0, n, p) # f(0) untruncated density\n",
    "\n",
    "    # general formula for lower trunc. distributions\n",
    "    cdf_y = (nbinom.cdf(y, n, p) - nbinom.cdf(0, n, p)) / (1 - f_zero)\n",
    "    ## error/input handling part two\n",
    "    # VERY IMPORTANT STEP: there might be numerical instabilities, which may lead to\n",
    "    # nbinomo.cdf(x) < nbinom.cdf(0)!!!!! this then leads to negative values of the cdf\n",
    "    # or nans in the log version (np.log(neg number))\n",
    "    cdf_y[cdf_y < 0] = 0\n",
    "    # set values to 0, if y <= 0 (y <=0 not allowed per definition of a 0 truncated count distribution)\n",
    "    cdf_y[y <= 0] = 0\n",
    "\n",
    "    # in case of log CDF\n",
    "    if log: \n",
    "        # ignore the 'RuntimeWarning: divide by zero encountered in log' warning (np.log(0))\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        log_cdf_y = np.log(cdf_y) # general formula for lower trunc. distributions\n",
    "        warnings.filterwarnings('default', category=RuntimeWarning)\n",
    "\n",
    "        if is_scalar:\n",
    "            return log_cdf_y[0]  # return scalar, if onedimensional array\n",
    "        else:\n",
    "            return log_cdf_y\n",
    "    # normal CDF\n",
    "    else:\n",
    "        if is_scalar:\n",
    "            return cdf_y[0]  \n",
    "        else:\n",
    "            return cdf_y\n",
    "    \n",
    "\n",
    "#log.p\tlogical; if TRUE, probabilities p are given as log(p)    \n",
    "def qnbinom_trunc(p, nNbinom, pNbinom, log_p=False):\n",
    "    ## calculation of the quantile\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if nbinom.pmf(0, nNbinom, pNbinom) == 0:\n",
    "        return nbinom.ppf(p, nNbinom, pNbinom) # if p=0, -1 is returned instead of 0 (0 is truncated)\n",
    "        # but this is not that important and is ignored here (because the 0 quantile does not make sense)\n",
    "    else:\n",
    "        # n and p have to be greater than zero and p <= 1\n",
    "        if nNbinom <= 0 or pNbinom <= 0 or pNbinom > 1:\n",
    "            if not isinstance(p, (list, np.ndarray)):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return np.full(len(p), np.nan)\n",
    "\n",
    "        # Convert p (quantile) to array if it's a scalar\n",
    "        if not isinstance(p, (list, np.ndarray)):\n",
    "            p = np.array([p])\n",
    "        elif isinstance(p, list):\n",
    "            p = np.array(p)\n",
    "        \n",
    "        n = len(p) # number of quantiles\n",
    "\n",
    "        # Set log-probabilities (lower tail)\n",
    "        if log_p:\n",
    "            logp = p\n",
    "        else:\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            logp = np.log(p)\n",
    "            warnings.filterwarnings('default', category=RuntimeWarning)\n",
    "        \n",
    "        # error handling/deal with special cases (outputs NA and Inf)\n",
    "        quantiles = np.full(n, np.nan)\n",
    "        na = np.isnan(logp) # nan <-> p < 0 -> return nan\n",
    "        neginf = np.isneginf(logp) # -inf <-> p = 0 -> return 0 (due to truncation, otherwise -1)\n",
    "        zero = logp == 0  # 0 <-> p = 1 -> return inf\n",
    "        aboveZero = logp > 0 # >0 <-> p > 1 -> return nan\n",
    "\n",
    "        # set quantile array if one of the restrictions is not fulfilled\n",
    "        quantiles[neginf] = 0\n",
    "        quantiles[zero] = np.inf\n",
    "        quantiles[aboveZero] = np.nan\n",
    "        \n",
    "        # mask array, true if conditions are fulfilled\n",
    "        mask = np.logical_not(np.logical_or(na, np.logical_or(neginf, np.logical_or(zero, aboveZero))))\n",
    "        validLogp = logp[mask]\n",
    "\n",
    "        if len(validLogp) == 0:\n",
    "            # Return output\n",
    "            if len(quantiles) == 1:\n",
    "                return quantiles[0] # if single quantile is handed over\n",
    "            else:\n",
    "                return quantiles\n",
    "\n",
    "        # find valid max value with mask\n",
    "        lp_max = np.max(validLogp)\n",
    "        p_max = np.exp(lp_max)\n",
    "\n",
    "        # calculate mean and variance out of n and p\n",
    "        mean = (nNbinom * (1 - pNbinom)) / pNbinom\n",
    "        var = (nNbinom * (1 - pNbinom)) / (pNbinom**2)\n",
    "\n",
    "        # find an adequate upper limit, starting from the extreme conservative chebychev inequality\n",
    "        upper = int(mean + np.sqrt(var/(1-np.exp(lp_max)))) #Chebychev inequality\n",
    "\n",
    "        # if upper < 1000 there is an log(0)=-inf with warning -> ignore this warning\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        # lower the upper limit (saves computation time)\n",
    "        while truncNbinomCdf(upper-1000, nNbinom, pNbinom, log=False) > p_max:\n",
    "            upper = upper - 1000\n",
    "\n",
    "        # after this section warnings are enabled again\n",
    "        warnings.filterwarnings('default', category=RuntimeWarning)\n",
    "\n",
    "        yarray = np.arange(1, int(upper)+1) # the y values for which the CDF is going to be calculated\n",
    "        logcdf = truncNbinomCdf(yarray, nNbinom, pNbinom) # calculate log CDF (faster computation time)\n",
    "\n",
    "        # Compute output\n",
    "        for i in range(n): # for all quantiles   \n",
    "            if not na[i] and not neginf[i] and not zero[i] and not aboveZero[i]:\n",
    "                    quantiles[i] = np.sum(logcdf < np.array(logp[i])) + 1 #+1 because 0 is truncated\n",
    "        \n",
    "        # Return output\n",
    "        if len(quantiles) == 1:\n",
    "            return quantiles[0] # if single quantile is handed over\n",
    "        else:\n",
    "            return quantiles\n",
    "\n",
    "# truncated poisson---------------------------------------------------\n",
    "# cdf of the truncated poisson distribution\n",
    "def truncPoisCdf(y, mu, log=True):\n",
    "    # values of lam <= 0 (0 truncation -> 0= not allowed) are not allowed. return nan\n",
    "    if mu <= 0:\n",
    "        if not isinstance(y, (list, np.ndarray)):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.full(len(y), np.nan)\n",
    "\n",
    "    is_scalar = np.isscalar(y)  # check if y is scalar or array\n",
    "\n",
    "    if is_scalar:\n",
    "        y = np.array([y])  # typecast scalar to onedimensional array\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "\n",
    "    f_zero = poisson.pmf(0, mu) # f(0) untruncated density\n",
    "\n",
    "    # general formula for lower trunc. distributions\n",
    "    cdf_y = (poisson.cdf(y, mu) - poisson.cdf(0, mu)) / (1 - f_zero)\n",
    "    # VERY IMPORTANT STEP: there might be numerical instabilities, which may lead to\n",
    "    # poisson.cdf(x) < poisson.cdf(0)!!!!! this then leads to negative values of the cdf\n",
    "    # or nans in the log version (np.log(neg number))\n",
    "    cdf_y[cdf_y < 0] = 0 # set negative values to zero\n",
    "    # set values to 0, if y <= 0 (y <=0 not allowed per definition of a 0 truncated count distribution)\n",
    "    cdf_y[y <= 0] = 0\n",
    "\n",
    "    # in case of log CDF\n",
    "    if log:\n",
    "        # ignore the 'RuntimeWarning: divide by zero encountered in log' warning (np.log(0))\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        log_cdf_y = np.log(cdf_y) # general formula for lower trunc. distributions\n",
    "        warnings.filterwarnings('default', category=RuntimeWarning)\n",
    "\n",
    "        if is_scalar:\n",
    "            return log_cdf_y[0]  # return scalar, if onedimensional array\n",
    "        else:\n",
    "            return log_cdf_y\n",
    "    # normal CDF    \n",
    "    else:\n",
    "        if is_scalar:\n",
    "            return cdf_y[0]\n",
    "        else:\n",
    "            return cdf_y\n",
    "        \n",
    "def log1mexp(x):\n",
    "    if np.any((x < 0) & (~np.isnan(x))):\n",
    "        raise ValueError(\"Inputs need to be non-negative!\")\n",
    "    return np.where(x <= np.log(2), np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))\n",
    "        \n",
    "\n",
    "#log.p\tlogical; if TRUE, probabilities p are given as log(p)    \n",
    "def qpois_trunc(p, lam, log_p=False):\n",
    "    ## calculation of the quantile\n",
    "    # if f(0)=0 no truncation is needed\n",
    "    if poisson.pmf(0, lam) == 0:\n",
    "        return poisson.ppf(p, lam)\n",
    "    else:\n",
    "        # values of lam <= 0 (0 truncation -> 0= not allowed) are not allowed. return nan\n",
    "        if lam <= 0:\n",
    "            if not isinstance(p, (list, np.ndarray)):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return np.full(len(p), np.nan)\n",
    "\n",
    "\n",
    "        # Convert p (quantile) to array if it's a scalar\n",
    "        if not isinstance(p, (list, np.ndarray)):\n",
    "            p = np.array([p])\n",
    "        elif isinstance(p, list):\n",
    "            p = np.array(p)\n",
    "        \n",
    "        n = len(p) # number of quantiles\n",
    "\n",
    "        # Set log-probabilities (lower tail)\n",
    "        if log_p:\n",
    "            logp = p\n",
    "        else:\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            logp = np.log(p)\n",
    "            warnings.filterwarnings('default', category=RuntimeWarning)\n",
    "        \n",
    "        # error handling/deal with special cases (outputs NA and Inf)\n",
    "        quantiles = np.full(n, np.nan)\n",
    "        na = np.isnan(logp) # nan <-> p < 0 -> return nan\n",
    "        neginf = np.isneginf(logp) # -inf <-> p = 0 -> return 0 (due to truncation, otherwise -1)\n",
    "        zero = logp == 0  # 0 <-> p = 1 -> return inf\n",
    "        aboveZero = logp > 0 # >0 <-> p > 1 -> return nan\n",
    "\n",
    "        # set quantile array if one of the restrictions is not fulfilled\n",
    "        quantiles[neginf] = 0\n",
    "        quantiles[zero] = np.inf\n",
    "        quantiles[aboveZero] = np.nan\n",
    "        \n",
    "        # mask array, true if conditions are fulfilled\n",
    "        mask = np.logical_not(np.logical_or(na, np.logical_or(neginf, np.logical_or(zero, aboveZero))))\n",
    "        validLogp = logp[mask]\n",
    "\n",
    "        if len(validLogp) == 0:\n",
    "            # Return output\n",
    "            if len(quantiles) == 1:\n",
    "                return quantiles[0] # if single quantile is handed over\n",
    "            else:\n",
    "                return quantiles\n",
    "\n",
    "        # find valid max value with mask\n",
    "        lp_max = np.max(validLogp)\n",
    "        p_max = np.exp(lp_max)\n",
    "\n",
    "        # find an adequate upper limit, starting from the extreme conservative chebychev inequality\n",
    "        upper = int(lam + np.sqrt(lam * np.exp(-log1mexp(-lp_max)))) #Chebychev inequality\n",
    "\n",
    "        # if upper < 1000 there is an log(0)=-inf with warning -> ignore this warning\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "        while truncPoisCdf(upper-1000, lam, log=False) > p_max:\n",
    "            upper = upper - 1000\n",
    "\n",
    "        # after this section warnings are enabled again\n",
    "        warnings.filterwarnings('default', category=RuntimeWarning)\n",
    "\n",
    "        yarray = np.arange(1, int(upper)+1) # the y values for which the CDF is going to be calculated\n",
    "        logcdf = truncPoisCdf(yarray, lam) # calculate log CDF (faster computation time)\n",
    "\n",
    "        # Compute output\n",
    "        for i in range(n): # for all quantiles   \n",
    "            if not na[i] and not neginf[i] and not zero[i] and not aboveZero[i]:\n",
    "                    quantiles[i] = np.sum(logcdf < np.array(logp[i])) + 1 #+1 because 0 is truncated\n",
    "        \n",
    "        # Return output\n",
    "        if len(quantiles) == 1:\n",
    "            return quantiles[0] # if single quantile is handed over\n",
    "        else:\n",
    "            return quantiles\n",
    "\n",
    "### function to compute distribution-------------------------------------------------\n",
    "def baseFatalModel_quantiles(featureSeries, quantiles, w=None, model='hurdle'):\n",
    "    # list to store quantiles \n",
    "    dummy_fatalities_list = []\n",
    "    # string to store model distribution\n",
    "    dist_string = ''\n",
    "\n",
    "    mean = None\n",
    "    var = None\n",
    "\n",
    "    numberQuantiles = len(quantiles)\n",
    "\n",
    "    # hurdle model\n",
    "    if model == 'hurdle':\n",
    "        if w == None:\n",
    "            \n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (featureSeries.value_counts().get(0, 0) / featureSeries.count())\n",
    "            # calculate mean and variance of the features\n",
    "            mean = pd.Series.mean(featureSeries[featureSeries != 0])\n",
    "            var = pd.Series.var(featureSeries[featureSeries != 0])\n",
    "\n",
    "        elif w <= 0:\n",
    "            raise ValueError(\"w has to be greater than zero\")\n",
    "        \n",
    "        else:\n",
    "            features = featureSeries.tail(w).loc[:,'ged_sb']\n",
    "            # calculate pt, i.e. the probability that y>0\n",
    "            p_t = 1 - (features.value_counts().get(0, 0) / features.count())\n",
    "            # calculate mean and variance of the features\n",
    "            mean = pd.Series.mean(features[features != 0])\n",
    "            var = pd.Series.var(features[features != 0])\n",
    "\n",
    "        # pd.Series.var or mean returns Nan in case of a passed series of length 1\n",
    "        if np.isnan(mean):\n",
    "            mean = 0\n",
    "        if np.isnan(var):\n",
    "            var = 0\n",
    "\n",
    "        # check if there are values above zero, otherwise no second component (trunc dist.) needed\n",
    "        if p_t > 0:\n",
    "            # component 1, y=0: Bernoulli\n",
    "            comp2_quantiles = [q for q in quantiles if q > (1-p_t)] #quantiles for the second component\n",
    "            removed_elements_length = numberQuantiles-len(comp2_quantiles)\n",
    "            zeros_array = np.zeros(removed_elements_length) #zero values that originate from the bernoulli dist\n",
    "\n",
    "            # component 2, y>0\n",
    "            if var != 0 and var > mean:\n",
    "                # calculate n (r) and p via average/variance\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "\n",
    "                trunc_nbinom_quantiles = qnbinom_trunc(comp2_quantiles, n, p)\n",
    "\n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_nbinom_quantiles)).tolist()\n",
    "                dist_string = 'BernoullitruncNbinom'\n",
    "\n",
    "            else:  # equivalent to all means and 0 < var <= mean (mean cant be 0, because of hurdle)\n",
    "                trunc_pois_quantiles = qpois_trunc(comp2_quantiles, mean)\n",
    "                \n",
    "                dummy_fatalities_list = np.concatenate((zeros_array, trunc_pois_quantiles)).tolist()\n",
    "                dist_string = 'BernoulliTruncPois'\n",
    "            \n",
    "        # p_t = 0 so no second component is needed    \n",
    "        else:\n",
    "            dummy_fatalities_list = [0] * numberQuantiles\n",
    "            dist_string = 'BernoulliHurdle'\n",
    "\n",
    "    # nbinom model\n",
    "    elif model == 'nbinom':\n",
    "        if w == None:\n",
    "             # calculate mean and variance of the features\n",
    "            mean = pd.Series.mean(featureSeries)\n",
    "            var = pd.Series.var(featureSeries)\n",
    "        elif w <= 0:\n",
    "            raise ValueError(\"w has to be greater than zero\")\n",
    "        else:\n",
    "            # calculate mean and variance of the features\n",
    "            mean = pd.Series.mean(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "            var = pd.Series.var(featureSeries.tail(w).loc[:,'ged_sb'])\n",
    "\n",
    "        if var != 0 and var > mean:\n",
    "                # calculate n (r) and p via average/variance\n",
    "                n = (mean**2) / (var - mean) # equivalent to r\n",
    "                p = mean / var\n",
    "\n",
    "                dummy_fatalities_list = nbinom.ppf(quantiles, n, p).tolist()\n",
    "                dist_string = 'NBinom'\n",
    "\n",
    "        elif mean == 0 and var == 0: # due to faster calculation\n",
    "                dummy_fatalities_list = [0] * numberQuantiles\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "        else: # equivalent to all means and 0 < var <= mean\n",
    "                dummy_fatalities_list = poisson.ppf(quantiles, mean).tolist()\n",
    "                dist_string = 'Pois'\n",
    "\n",
    "    return {'fatalities': dummy_fatalities_list, 'dist': dist_string, 'mean': mean, 'var': var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last feature year\n",
    "feature_year = '2024'\n",
    "\n",
    "prediction_year = '2024/25'\n",
    "\n",
    "# path to the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# read feature dataset\n",
    "relative_path_features = os.path.join('..','..', 'data', 'cm_features.parquet')\n",
    "path_features = os.path.join(current_dir, relative_path_features)\n",
    "\n",
    "feature_data_toApr2024 = pd.read_parquet(path_features, engine='pyarrow')\n",
    "feature_data_toApr2024.set_index(['month_id', 'country_id'], inplace=True)\n",
    "\n",
    "country_list = sorted(feature_data_toApr2024.index.get_level_values('country_id').unique().tolist())\n",
    "\n",
    "# country group list of all years\n",
    "country_feature_group_list = []\n",
    "\n",
    "# fill list \n",
    "country_feature_group_list.append(feature_data_toApr2024.groupby('country_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of the countries for which a prediction is requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_path_countrylist = os.path.join('..', '..', 'data', 'country_list.csv')\n",
    "path_countrylist = os.path.join(current_dir, relative_path_countrylist)\n",
    "\n",
    "# CSV-Datei einlesen und als Pandas-Datensatz speichern\n",
    "countryList_prediction = pd.read_csv(path_countrylist)\n",
    "country_list_views = countryList_prediction.loc[:,'country_id'].values.tolist() \n",
    "\n",
    "month_list = []\n",
    "countries_to_remove = []\n",
    "for country_id in country_list:\n",
    "\n",
    "    if country_id in country_list_views:\n",
    "        feature_data = country_feature_group_list[0].get_group(country_id)\n",
    "\n",
    "        # numbers of months from the feature dataset\n",
    "        month_list_feature_data_original = feature_data.index.get_level_values('month_id').tolist()\n",
    "        number_months_feature_data = len(month_list_feature_data_original) \n",
    "        \n",
    "    else:\n",
    "        countries_to_remove.append(country_id)\n",
    "\n",
    "country_list = list(set(country_list) - set(countries_to_remove))\n",
    "month_list.sort()\n",
    "len(country_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1-4\n",
    "\n",
    "Optimize **w** (through the CRPS) regarding\n",
    "|            | datasets    | countries   | prediction windows |\n",
    "|------------|-------------|-------------|--------------------|\n",
    "| baseline 1 | all         | all         | all                |\n",
    "| baseline 2 | all         | inidvidual  | all                |\n",
    "| baseline 3 | all         | all         | individual         |\n",
    "| baseline 4 | all         | inidvidual  | individual         |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The minimization is based on calculating the quantiles for each country, w and year (of the four datasets).\n",
    "#### List of representative countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample_list = [1, 57, 235, 13, 121, 223, 220]\n",
    "#country_list = [x for x in country_list if x in subsample_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:31<00:00,  3.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# modify country_list so that it contains only country_ids \n",
    "# that have at least the last n months of observations in the last dataset (2020)!\n",
    "numberMonths_toApr24 = 138 # 120 = 5*12 (5 jahre für 2017) + 5*12 (jedes Jahr 12 Monate mehr also 2020 10 Jahre) + 12 (23) + 6 (Oktober bis April)\n",
    "#ABER: um konsistent zu bleiben wird für jedes Jahr (jeden to_octX Datensatz) nur die letzten 5 Jahre verwendet!!!\n",
    "\n",
    "\n",
    "s_prediction_list = list(range(3, 15))\n",
    "\n",
    "number_countries = len(country_list)\n",
    "number_w = len(window_list)\n",
    "\n",
    "# lists for the estimation of the distribution\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "string_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles] # sting values of the quantiles\n",
    "\n",
    "# last month of the dataframe as reference for the moving prediction windows\n",
    "last_month = feature_data_toApr2024.index.get_level_values('month_id').tolist()[-1]\n",
    "\n",
    "# list to store the estimations/predictions for each w\n",
    "baseline_estimate_list = []\n",
    "\n",
    "# loop through windows\n",
    "for i in tqdm(range(number_w)):\n",
    "\n",
    "    w = window_list[i] # current window\n",
    "    baseline_estimate_list.append({'window':w, \n",
    "                                'country_predict_list':[{'country_id':country, 'predictionWindowsN':[]} for country in country_list]})\n",
    "    \n",
    "    #calculate the number of subsets, that are used to estimate the distribution and validate it via 12 months of actuals \n",
    "    # the number is dependent of the actual w. E.g. with the maximal w (e.g. 24): if w=24, actuals are 12 months (starting with s=3 to s=14) \n",
    "    # -> 24 + 2 + 12 = 39 observations of ged_sb per window\n",
    "    # so if the dataset has 120 observations there are 120 - 38 = 82 shiftable windows for 2020\n",
    "    numberWindows = numberMonths_toApr24 - (w + 2 + 12)\n",
    "\n",
    "    windowLength = w + 2 + 12 # length of the individual window for the current w\n",
    "    \n",
    "    # loop through all countries\n",
    "    for index in range(number_countries):\n",
    "\n",
    "        country = country_list[index]\n",
    "\n",
    "        features = country_feature_group_list[-1].get_group(country) # features of country\n",
    "        \n",
    "\n",
    "        # loop through all X equal parts of the feature dataset (traindata length w, actuals is vector of the next t+3 till t+12 observations)\n",
    "        for j in range(numberWindows):\n",
    "            starting_month_window = last_month - windowLength + 1 - numberWindows + 1  + j\n",
    "            ending_month_window = starting_month_window + w - 1\n",
    "\n",
    "            starting_month_actuals = ending_month_window + 3\n",
    "            ending_month_actuals = starting_month_actuals + 11\n",
    "            \n",
    "            window_features = features.loc[(slice(starting_month_window, ending_month_window), slice(None)), 'ged_sb']\n",
    "            window_actuals = features.loc[(slice(starting_month_actuals, ending_month_actuals), slice(None)), 'ged_sb']\n",
    "\n",
    "            #predict = nBinom_quantiles(window_features, 'None', quantiles)\n",
    "            predict = baseFatalModel_quantiles(window_features, quantiles, model=estimModel)\n",
    "            \n",
    "            baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'].append(\n",
    "                [{'country_id':country, 'w':w, 'dist':predict['dist'], \n",
    "                'mean':predict['mean'], 'var':predict['var'], 'first_month_feature':starting_month_window, \n",
    "                'quantile':string_quantile_list, 'fatalities':predict['fatalities']}, \n",
    "                {'s':s_prediction_list, \n",
    "                    'month_id': window_actuals.index.get_level_values('month_id'),\n",
    "                    'unreal_actuals':window_actuals.values}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                     w                        country         rollingWindow|predicton/actuals\n",
    "#baseline_estimate_list[0]['country_predict_list'][177]['predictionWindowsN'][8][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the average CRPS over all indiviual moving windows per w and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [16:41<00:00, 83.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# lists to store all crps values\n",
    "baseline_crps_list_toApr2024 = [\n",
    "    {\n",
    "        'country_id': country,\n",
    "        'baseline': [\n",
    "            {'s': s, 'w': [], 'CRPS': []}\n",
    "            for s in s_prediction_list\n",
    "        ]\n",
    "    }\n",
    "    for country in country_list\n",
    "]\n",
    "baseline_crps_list_toApr2024 = copy.deepcopy(baseline_crps_list_toApr2024)\n",
    "\n",
    "# number of prediction windows\n",
    "number_s = len(s_prediction_list)\n",
    "\n",
    "# fill lists with crps calculations\n",
    "for s in tqdm(s_prediction_list):\n",
    "\n",
    "    for index in range(number_countries):\n",
    "        country = country_list[index]\n",
    "            \n",
    "        for i in range(number_w):\n",
    "            w = window_list[i]\n",
    "            dummy_crps_list = [] \n",
    "\n",
    "            # loop over all subset windows of the country and w \n",
    "            for j in range(len(baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'])):\n",
    "\n",
    "                distribution = baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'][j][0]['fatalities']\n",
    "                actual = baseline_estimate_list[i]['country_predict_list'][index]['predictionWindowsN'][j][1]['unreal_actuals'][s-3]\n",
    "\n",
    "                crps = pscore(np.array(distribution),actual).compute()[0]\n",
    "\n",
    "                dummy_crps_list.append(crps)\n",
    "\n",
    "            \n",
    "            # dataframe toApr2024\n",
    "            baseline_crps_list_toApr2024[index]['baseline'][s-3]['w'].append(w)\n",
    "            baseline_crps_list_toApr2024[index]['baseline'][s-3]['CRPS'].append(np.mean(dummy_crps_list))\n",
    "\n",
    "task2_baseline_list = [baseline_crps_list_toApr2024]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimization\n",
    "'w_minimization_list' contains the minimal w's for the different baselines for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the results of the minimal w's\n",
    "w_minimization_list = [{'predictionYear':prediction_year, 'minWData':[]}]\n",
    "\n",
    "# list to store the list to compute the minimal w's\n",
    "w_compute_list = [{'predictionYear':prediction_year, 'data':[]}]\n",
    "\n",
    "# loop over the four different datasets to predict (18-21)\n",
    "\n",
    "v1_baseline_crps_dict = {'w':[],'CRPS':[]}\n",
    "v2_baseline_crps_list = [{'country_id': country, 'baseline': {'w':[],'CRPS':[]}} for country in country_list]\n",
    "v3_baseline_crps_list = [{'s':s,'w':[],'CRPS':[]} for s in s_prediction_list]\n",
    "\n",
    "## baseline v1---------------------------------------------------------------------------\n",
    "# loop over w\n",
    "for j in range(number_w):\n",
    "    w = window_list[j]\n",
    "    dummy_crps_v1_list = []\n",
    "    # loop over countries\n",
    "    for i in range(number_countries):\n",
    "        # loop over prediction windows s\n",
    "        for k in range(number_s):\n",
    "            dummy_crps_v1_list.append(task2_baseline_list[0][i]['baseline'][k]['CRPS'][j])\n",
    "    v1_baseline_crps_dict['w'].append(w)\n",
    "    v1_baseline_crps_dict['CRPS'].append(np.mean(dummy_crps_v1_list))\n",
    "\n",
    "v1_baseline_crps = pd.DataFrame(v1_baseline_crps_dict)\n",
    "\n",
    "w_compute_list[0]['data'].append(v1_baseline_crps)\n",
    "\n",
    "v1_baseline_crps = v1_baseline_crps[v1_baseline_crps.CRPS == v1_baseline_crps.loc[:,'CRPS'].min()]\n",
    "v1_baseline_crps.set_index(pd.Index(range(len(v1_baseline_crps))), inplace=True)\n",
    "    \n",
    "w_minimization_list[0]['minWData'].append(v1_baseline_crps)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "## baseline v2----------------------------------------------------------------------------\n",
    "# list for baseline v2\n",
    "for i in range(number_countries):\n",
    "    for j in range(number_w):\n",
    "        w = window_list[j]\n",
    "        dummy_crps_v2_list = []\n",
    "        for k in range(number_s):\n",
    "            dummy_crps_v2_list.append(task2_baseline_list[0][i]['baseline'][k]['CRPS'][j])\n",
    "        v2_baseline_crps_list[i]['baseline']['w'].append(w)\n",
    "        v2_baseline_crps_list[i]['baseline']['CRPS'].append(np.mean(dummy_crps_v2_list))\n",
    "    \n",
    "# dataframe with the w that minimizes the CRPS for every country (v2)\n",
    "data_v2 = {\n",
    "    'country_id':[],\n",
    "    'w':[],\n",
    "    'CRPS':[]\n",
    "}\n",
    "for i in range(len(v2_baseline_crps_list)):\n",
    "    # get the index of the minimal CRPS value\n",
    "    min_index = v2_baseline_crps_list[i]['baseline']['CRPS'].index(min(v2_baseline_crps_list[i]['baseline']['CRPS']))\n",
    "    \n",
    "    # store values in dict\n",
    "    data_v2['country_id'].append(v2_baseline_crps_list[i]['country_id'])\n",
    "    data_v2['w'].append(v2_baseline_crps_list[i]['baseline']['w'][min_index])\n",
    "    data_v2['CRPS'].append(v2_baseline_crps_list[i]['baseline']['CRPS'][min_index])\n",
    "    \n",
    "v2_baseline_crps = pd.DataFrame(data_v2)\n",
    "w_minimization_list[0]['minWData'].append(v2_baseline_crps)\n",
    "w_compute_list[0]['data'].append(v2_baseline_crps_list)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## baseline v3---------------------------------------------------------------------------\n",
    "for s_index in range(number_s):\n",
    "    dummy_crps_v3_list = []\n",
    "    s = s_prediction_list[s_index]\n",
    "    for w_index in range(number_w):\n",
    "        w = window_list[w_index]\n",
    "        for i in range(number_countries):\n",
    "            dummy_crps_v3_list.append(task2_baseline_list[0][i]['baseline'][s_index]['CRPS'][w_index])\n",
    "        v3_baseline_crps_list[s_index]['w'].append(w)\n",
    "        v3_baseline_crps_list[s_index]['CRPS'].append(np.mean(dummy_crps_v3_list))\n",
    "\n",
    "# dataframe with the w that minimize the CRPS for each prediction window s\n",
    "data_v3 = {\n",
    "    's':[],\n",
    "    'w':[],\n",
    "    'CRPS':[]\n",
    "}\n",
    "# length of the v3_baseline list is the number of prediction windows\n",
    "for i in range(len(v3_baseline_crps_list)):\n",
    "    s = s_prediction_list[i]\n",
    "    # get the index of the minimal CRPS value\n",
    "    min_index = v3_baseline_crps_list[i]['CRPS'].index(min(v3_baseline_crps_list[i]['CRPS']))\n",
    "\n",
    "    # store values in dict\n",
    "    data_v3['s'].append(s)\n",
    "    data_v3['w'].append(v3_baseline_crps_list[i]['w'][min_index])\n",
    "    data_v3['CRPS'].append(v3_baseline_crps_list[i]['CRPS'][min_index])\n",
    "\n",
    "v3_baseline_crps = pd.DataFrame(data_v3)\n",
    "\n",
    "w_minimization_list[0]['minWData'].append(v3_baseline_crps)\n",
    "w_compute_list[0]['data'].append(v3_baseline_crps_list)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "## baseline v4---------------------------------------------------------------------------\n",
    "v4_baseline_crps = [{'country_id':country,\n",
    "                    's':[],\n",
    "                    'w':[],\n",
    "                    'CRPS':[]\n",
    "                    } for country in country_list]\n",
    "\n",
    "# loop over all countries\n",
    "for i in range(len(task2_baseline_list[0])):\n",
    "    # loop over all prediction windows\n",
    "    for s_index in range(number_s):\n",
    "        s = s_prediction_list[s_index]\n",
    "        # get the index of the minimal CRPS value\n",
    "        min_index = task2_baseline_list[0][i]['baseline'][s_index]['CRPS'].index(min(task2_baseline_list[0][i]['baseline'][s_index]['CRPS']))\n",
    "    \n",
    "        # store values in dict\n",
    "        v4_baseline_crps[i]['s'].append(s)\n",
    "        v4_baseline_crps[i]['w'].append(task2_baseline_list[0][i]['baseline'][s_index]['w'][min_index])\n",
    "        v4_baseline_crps[i]['CRPS'].append(task2_baseline_list[0][i]['baseline'][s_index]['CRPS'][min_index])\n",
    "\n",
    "    v4_baseline_crps[i] = pd.DataFrame(v4_baseline_crps[i])\n",
    "\n",
    "w_minimization_list[0]['minWData'].append(v4_baseline_crps)\n",
    "w_compute_list[0]['data'].append(task2_baseline_list[0])\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>w</th>\n",
       "      <th>CRPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0.657637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>2694.937713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220</td>\n",
       "      <td>24</td>\n",
       "      <td>763.362668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>223</td>\n",
       "      <td>8</td>\n",
       "      <td>11.720113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>235</td>\n",
       "      <td>7</td>\n",
       "      <td>0.481448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_id   w         CRPS\n",
       "0           1   2     0.000000\n",
       "1          13  15     0.657637\n",
       "2          57   9  2694.937713\n",
       "3         121   2     0.000000\n",
       "4         220  24   763.362668\n",
       "5         223   8    11.720113\n",
       "6         235   7     0.481448"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                dataset/year |baseline method|country (only baseline 4)\n",
    "w_minimization_list[0]['minWData'][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with the estimated baseline models\n",
    "With the w values for the four different baselines computed above the prediction is calculated and evaluated in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the predictions for each country\n",
    "baseline_prediction_list = [[{'country_id': country, 'base': 1, 'prediction': {prediction_year: []}} for country in country_list],\n",
    "                            [{'country_id': country, 'base': 2, 'prediction': {prediction_year: []}} for country in country_list],\n",
    "                            [{'country_id': country, 'base': 3, 'prediction': {prediction_year: []}} for country in country_list],\n",
    "                            [{'country_id': country, 'base': 4, 'prediction': {prediction_year: []}} for country in country_list]]\n",
    "\n",
    "quantiles = np.arange(0.001, 0.9999, 0.001)\n",
    "quantiles = [round(q, 3) for q in quantiles] # due to binary inaccuracies\n",
    "string_quantile_list = [f\"{round(q * 100, 1)}%\" for q in quantiles]\n",
    "\n",
    "\n",
    "# loop through all countries (that are present in each dataset)\n",
    "for index in range(number_countries):\n",
    "    country = country_list[index]\n",
    "    \n",
    "    features = country_feature_group_list[0].get_group(country) # features of country in dataset i\n",
    "\n",
    "    # loop over the four different baseline minimization methods\n",
    "    for j in range(len(w_minimization_list[0]['minWData'])):\n",
    "\n",
    "        # baseline 1\n",
    "        if j == 0:\n",
    "            w = w_minimization_list[0]['minWData'][j].loc[0,'w'] # use the w obtained by minimization on the feature dataset\n",
    "            #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "            fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "            baseline_prediction_list[j][index]['prediction'][prediction_year].append({'s':s_prediction_list, 'w':w, \n",
    "                                                                                'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                'var':fit['var'], \n",
    "                                                                                'quantile':string_quantile_list, \n",
    "                                                                                'fatalities':fit['fatalities']})\n",
    "\n",
    "        # baseline 2\n",
    "        elif j == 1:\n",
    "            if country == w_minimization_list[0]['minWData'][j].loc[index,'country_id']:\n",
    "                w = w_minimization_list[0]['minWData'][j].loc[index,'w']\n",
    "                #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                baseline_prediction_list[j][index]['prediction'][prediction_year].append({'s':s_prediction_list, 'w':w, \n",
    "                                                                                    'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                    'var':fit['var'], \n",
    "                                                                                    'quantile':string_quantile_list, \n",
    "                                                                                    'fatalities':fit['fatalities']})\n",
    "            else:\n",
    "                print('Stopp')\n",
    "                break\n",
    "\n",
    "        # baseline 3\n",
    "        elif j == 2:\n",
    "            for s in s_prediction_list:\n",
    "                w = w_minimization_list[0]['minWData'][j].loc[s-3,'w']\n",
    "                #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                baseline_prediction_list[j][index]['prediction'][prediction_year].append({'s':s, 'w':w, \n",
    "                                                                                    'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                    'var':fit['var'], \n",
    "                                                                                    'quantile':string_quantile_list, \n",
    "                                                                                    'fatalities':fit['fatalities']})\n",
    "\n",
    "        # baseline 4\n",
    "        elif j == 3:\n",
    "            if country == w_minimization_list[0]['minWData'][j][index].loc[0,'country_id']:\n",
    "                for s in s_prediction_list:\n",
    "                    w = w_minimization_list[0]['minWData'][j][index].loc[s-3,'w']\n",
    "                    #fit = nBinom_quantiles(features, w, quantiles) # calculate the quantiles for the w\n",
    "                    fit = baseFatalModel_quantiles(features, quantiles, w=w, model=estimModel)\n",
    "\n",
    "                    baseline_prediction_list[j][index]['prediction'][prediction_year].append({'s':s, 'w':w, \n",
    "                                                                                        'dist':fit['dist'], 'mean':fit['mean'],\n",
    "                                                                                        'var':fit['var'], \n",
    "                                                                                        'quantile':string_quantile_list, \n",
    "                                                                                        'fatalities':fit['fatalities']})\n",
    "            else:\n",
    "                print('Stopp')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0,\n",
       " 30.0,\n",
       " 31.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 36.0,\n",
       " 37.0,\n",
       " 37.0,\n",
       " 38.0,\n",
       " 39.0,\n",
       " 40.0,\n",
       " 41.0,\n",
       " 42.0,\n",
       " 43.0,\n",
       " 44.0,\n",
       " 45.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 49.0,\n",
       " 51.0,\n",
       " 52.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 57.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 73.0,\n",
       " 74.0,\n",
       " 76.0,\n",
       " 78.0,\n",
       " 80.0,\n",
       " 81.0,\n",
       " 83.0,\n",
       " 85.0,\n",
       " 87.0,\n",
       " 89.0,\n",
       " 91.0,\n",
       " 93.0,\n",
       " 95.0,\n",
       " 97.0,\n",
       " 99.0,\n",
       " 101.0,\n",
       " 104.0,\n",
       " 106.0,\n",
       " 108.0,\n",
       " 111.0,\n",
       " 113.0,\n",
       " 116.0,\n",
       " 118.0,\n",
       " 121.0,\n",
       " 123.0,\n",
       " 126.0,\n",
       " 129.0,\n",
       " 132.0,\n",
       " 134.0,\n",
       " 137.0,\n",
       " 140.0,\n",
       " 143.0,\n",
       " 146.0,\n",
       " 150.0,\n",
       " 153.0,\n",
       " 156.0,\n",
       " 159.0,\n",
       " 163.0,\n",
       " 166.0,\n",
       " 170.0,\n",
       " 174.0,\n",
       " 177.0,\n",
       " 181.0,\n",
       " 185.0,\n",
       " 189.0,\n",
       " 193.0,\n",
       " 197.0,\n",
       " 201.0,\n",
       " 205.0,\n",
       " 209.0,\n",
       " 214.0,\n",
       " 218.0,\n",
       " 223.0,\n",
       " 228.0,\n",
       " 232.0,\n",
       " 237.0,\n",
       " 242.0,\n",
       " 247.0,\n",
       " 252.0,\n",
       " 257.0,\n",
       " 263.0,\n",
       " 268.0,\n",
       " 274.0,\n",
       " 279.0,\n",
       " 285.0,\n",
       " 291.0,\n",
       " 297.0,\n",
       " 303.0,\n",
       " 309.0,\n",
       " 316.0,\n",
       " 322.0,\n",
       " 329.0,\n",
       " 335.0,\n",
       " 342.0,\n",
       " 349.0,\n",
       " 356.0,\n",
       " 363.0,\n",
       " 371.0,\n",
       " 378.0,\n",
       " 386.0,\n",
       " 393.0,\n",
       " 401.0,\n",
       " 409.0,\n",
       " 418.0,\n",
       " 426.0,\n",
       " 434.0,\n",
       " 443.0,\n",
       " 452.0,\n",
       " 461.0,\n",
       " 470.0,\n",
       " 479.0,\n",
       " 489.0,\n",
       " 498.0,\n",
       " 508.0,\n",
       " 518.0,\n",
       " 529.0,\n",
       " 539.0,\n",
       " 550.0,\n",
       " 560.0,\n",
       " 571.0,\n",
       " 582.0,\n",
       " 594.0,\n",
       " 605.0,\n",
       " 617.0,\n",
       " 629.0,\n",
       " 642.0,\n",
       " 654.0,\n",
       " 667.0,\n",
       " 680.0,\n",
       " 693.0,\n",
       " 706.0,\n",
       " 720.0,\n",
       " 734.0,\n",
       " 748.0,\n",
       " 762.0,\n",
       " 777.0,\n",
       " 792.0,\n",
       " 807.0,\n",
       " 822.0,\n",
       " 838.0,\n",
       " 854.0,\n",
       " 870.0,\n",
       " 887.0,\n",
       " 904.0,\n",
       " 921.0,\n",
       " 938.0,\n",
       " 956.0,\n",
       " 974.0,\n",
       " 993.0,\n",
       " 1011.0,\n",
       " 1030.0,\n",
       " 1050.0,\n",
       " 1070.0,\n",
       " 1090.0,\n",
       " 1110.0,\n",
       " 1131.0,\n",
       " 1152.0,\n",
       " 1174.0,\n",
       " 1196.0,\n",
       " 1218.0,\n",
       " 1241.0,\n",
       " 1264.0,\n",
       " 1287.0,\n",
       " 1311.0,\n",
       " 1336.0,\n",
       " 1360.0,\n",
       " 1386.0,\n",
       " 1411.0,\n",
       " 1437.0,\n",
       " 1464.0,\n",
       " 1491.0,\n",
       " 1518.0,\n",
       " 1546.0,\n",
       " 1575.0,\n",
       " 1604.0,\n",
       " 1633.0,\n",
       " 1663.0,\n",
       " 1694.0,\n",
       " 1725.0,\n",
       " 1756.0,\n",
       " 1788.0,\n",
       " 1821.0,\n",
       " 1854.0,\n",
       " 1888.0,\n",
       " 1922.0,\n",
       " 1957.0,\n",
       " 1993.0,\n",
       " 2029.0,\n",
       " 2066.0,\n",
       " 2103.0,\n",
       " 2141.0,\n",
       " 2180.0,\n",
       " 2219.0,\n",
       " 2259.0,\n",
       " 2300.0,\n",
       " 2341.0,\n",
       " 2383.0,\n",
       " 2426.0,\n",
       " 2470.0,\n",
       " 2514.0,\n",
       " 2559.0,\n",
       " 2605.0,\n",
       " 2652.0,\n",
       " 2699.0,\n",
       " 2748.0,\n",
       " 2797.0,\n",
       " 2847.0,\n",
       " 2897.0,\n",
       " 2949.0,\n",
       " 3002.0,\n",
       " 3055.0,\n",
       " 3109.0,\n",
       " 3165.0,\n",
       " 3221.0,\n",
       " 3278.0,\n",
       " 3336.0,\n",
       " 3395.0,\n",
       " 3455.0,\n",
       " 3517.0,\n",
       " 3579.0,\n",
       " 3642.0,\n",
       " 3706.0,\n",
       " 3772.0,\n",
       " 3838.0,\n",
       " 3906.0,\n",
       " 3975.0,\n",
       " 4045.0,\n",
       " 4116.0,\n",
       " 4189.0,\n",
       " 4262.0,\n",
       " 4337.0,\n",
       " 4414.0,\n",
       " 4491.0,\n",
       " 4570.0,\n",
       " 4650.0,\n",
       " 4732.0,\n",
       " 4815.0,\n",
       " 4899.0,\n",
       " 4985.0,\n",
       " 5072.0,\n",
       " 5161.0,\n",
       " 5252.0,\n",
       " 5344.0,\n",
       " 5437.0,\n",
       " 5532.0,\n",
       " 5629.0,\n",
       " 5727.0,\n",
       " 5827.0,\n",
       " 5929.0,\n",
       " 6033.0,\n",
       " 6138.0,\n",
       " 6246.0,\n",
       " 6355.0,\n",
       " 6466.0,\n",
       " 6579.0,\n",
       " 6694.0,\n",
       " 6810.0,\n",
       " 6929.0,\n",
       " 7050.0,\n",
       " 7174.0,\n",
       " 7299.0,\n",
       " 7426.0,\n",
       " 7556.0,\n",
       " 7688.0,\n",
       " 7822.0,\n",
       " 7959.0,\n",
       " 8098.0,\n",
       " 8240.0,\n",
       " 8384.0,\n",
       " 8531.0,\n",
       " 8680.0,\n",
       " 8832.0,\n",
       " 8986.0,\n",
       " 9144.0,\n",
       " 9304.0,\n",
       " 9467.0,\n",
       " 9633.0,\n",
       " 9802.0,\n",
       " 9974.0,\n",
       " 10150.0,\n",
       " 10328.0,\n",
       " 10510.0,\n",
       " 10695.0,\n",
       " 10883.0,\n",
       " 11075.0,\n",
       " 11270.0,\n",
       " 11469.0,\n",
       " 11672.0,\n",
       " 11879.0,\n",
       " 12089.0,\n",
       " 12303.0,\n",
       " 12521.0,\n",
       " 12744.0,\n",
       " 12970.0,\n",
       " 13201.0,\n",
       " 13436.0,\n",
       " 13676.0,\n",
       " 13920.0,\n",
       " 14169.0,\n",
       " 14423.0,\n",
       " 14682.0,\n",
       " 14945.0,\n",
       " 15214.0,\n",
       " 15488.0,\n",
       " 15768.0,\n",
       " 16053.0,\n",
       " 16344.0,\n",
       " 16640.0,\n",
       " 16943.0,\n",
       " 17251.0,\n",
       " 17566.0,\n",
       " 17887.0,\n",
       " 18215.0,\n",
       " 18550.0,\n",
       " 18891.0,\n",
       " 19240.0,\n",
       " 19595.0,\n",
       " 19959.0,\n",
       " 20330.0,\n",
       " 20709.0,\n",
       " 21096.0,\n",
       " 21491.0,\n",
       " 21895.0,\n",
       " 22307.0,\n",
       " 22729.0,\n",
       " 23160.0,\n",
       " 23601.0,\n",
       " 24052.0,\n",
       " 24512.0,\n",
       " 24984.0,\n",
       " 25466.0,\n",
       " 25959.0,\n",
       " 26464.0,\n",
       " 26980.0,\n",
       " 27509.0,\n",
       " 28051.0,\n",
       " 28606.0,\n",
       " 29174.0,\n",
       " 29756.0,\n",
       " 30353.0,\n",
       " 30965.0,\n",
       " 31592.0,\n",
       " 32235.0,\n",
       " 32895.0,\n",
       " 33573.0,\n",
       " 34268.0,\n",
       " 34982.0,\n",
       " 35715.0,\n",
       " 36469.0,\n",
       " 37243.0,\n",
       " 38040.0,\n",
       " 38859.0,\n",
       " 39702.0,\n",
       " 40569.0,\n",
       " 41463.0,\n",
       " 42383.0,\n",
       " 43332.0,\n",
       " 44311.0,\n",
       " 45320.0,\n",
       " 46362.0,\n",
       " 47439.0,\n",
       " 48551.0,\n",
       " 49702.0,\n",
       " 50892.0,\n",
       " 52124.0,\n",
       " 53401.0,\n",
       " 54725.0,\n",
       " 56100.0,\n",
       " 57527.0,\n",
       " 59010.0,\n",
       " 60554.0,\n",
       " 62162.0,\n",
       " 63838.0,\n",
       " 65588.0,\n",
       " 67417.0,\n",
       " 69331.0,\n",
       " 71336.0,\n",
       " 73441.0,\n",
       " 75653.0,\n",
       " 77983.0,\n",
       " 80442.0,\n",
       " 83041.0,\n",
       " 85795.0,\n",
       " 88722.0,\n",
       " 91839.0,\n",
       " 95171.0,\n",
       " 98744.0,\n",
       " 102591.0,\n",
       " 106751.0,\n",
       " 111272.0,\n",
       " 116215.0,\n",
       " 121655.0,\n",
       " 127692.0,\n",
       " 134454.0,\n",
       " 142120.0,\n",
       " 150940.0,\n",
       " 161281.0,\n",
       " 173718.0,\n",
       " 189220.0,\n",
       " 209622.0,\n",
       " 239080.0,\n",
       " 291045.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#          baseline method|country                  s-3 (only index 0 for baseline 1 and 2 due to the non minimizing s)\n",
    "#baseline_prediction_list[2][2]['prediction'][prediction_year][11]['fatalities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FinalTask2_baseline_predct_hurdleWmax24.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save variables in joblib files\n",
    "variable_string = str(estimModel)+'Wmax'+str(max_w)\n",
    "file1name = 'FinalTask1_baseline_estim_' + variable_string + '.joblib'\n",
    "file2name = 'FinalTask1_baseline_predct_' + variable_string + '.joblib'\n",
    "\n",
    "dump([country_list, baseline_estimate_list], \n",
    "       file1name)\n",
    "\n",
    "dump([task2_baseline_list, w_minimization_list, baseline_prediction_list], \n",
    "       file2name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
